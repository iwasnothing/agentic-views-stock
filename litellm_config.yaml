# LiteLLM Proxy configuration
# Docs: https://docs.litellm.ai/docs/proxy/configs
#
# The backend connects to this proxy via OPENAI_BASE_URL=http://litellm:4000/v1
# Add your models below. The proxy exposes an OpenAI-compatible API.

model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY

litellm_settings:
  drop_params: true
  set_verbose: false

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
